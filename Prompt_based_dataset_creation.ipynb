{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_project_II.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBt43Gyqpht9"
      },
      "source": [
        "# Author: Pavan Kumar Raja"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-PswXJnm0f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bbc413f-df2a-4b55-dd90-bf55966a229b"
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwzDUhALJ7MG"
      },
      "source": [
        "# os.makedirs('drive/MyDrive/NLP_PROJECT/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbXmk4rDoXg1"
      },
      "source": [
        "os.chdir('drive/MyDrive/NLP_PROJECT/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srkr0H_J_NFx"
      },
      "source": [
        "# T0pp is too big to download and use 48GB, T0_3B don't give good performance."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTTpCmT2po16"
      },
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0_3B\")\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0_3B\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTKLk_N3wyuT"
      },
      "source": [
        "# import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhhaihKlvIl8"
      },
      "source": [
        "# sentence_tokens = [\"weights\", \"rope\", \"pully\", \"man\", \"women\"]\n",
        "# for i in range(5):\n",
        "#     inputs = tokenizer.encode(f\"Ask two questions on topic {random.choice(sentence_tokens)}, Output generated questions as Question1, Question2\", return_tensors=\"pt\")\n",
        "#     outputs = model.generate(inputs)\n",
        "#     for x in outputs:\n",
        "#         print(tokenizer.decode(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6E-XopU20Bg"
      },
      "source": [
        "# My api keys are revoked.\n",
        "!pip install banana-dev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP7b5HOdtL08"
      },
      "source": [
        "import csv\n",
        "import banana_dev as banana\n",
        "banana_api = {Banana API}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TjVsiaLiwYA"
      },
      "source": [
        "import csv\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import banana_dev as banana\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Limiting hypothesis to maximum 5 sentences, in case there is a break before complition of a sentence.\n",
        "def process_sentences(sentence):\n",
        "    if sentence.endswith('.'): return sentence\n",
        "    sentences = sentence.split('.')\n",
        "    if len(sentences) == 1:\n",
        "        sentence = sentences[0]\n",
        "    else:\n",
        "        sentences = sentences[:-1]\n",
        "        sentences = sentences[:5]\n",
        "        sentence = sentences[0] + '.'.join(sentences[1:]) + '.'\n",
        "    return sentence\n",
        "\n",
        "# Premise hypothesis - text\n",
        "def premise_hypothesis_normal():\n",
        "    model_key = \"t0pp\"\n",
        "    model_parameters = {\n",
        "    \"text\": \"Generate a premise.\",\n",
        "    \"max_length\": 256,\n",
        "    }\n",
        "    out1 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence1 = process_sentences(out1['choices'][-1]['output'][-1])\n",
        "\n",
        "    model_parameters = {\n",
        "        \"text\": f\"Generatec a hypothesis form the premise. Premise: {sentence1}\",\n",
        "        \"max_length\": 64,\n",
        "    }\n",
        "    out2 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence2 = process_sentences(out2['choices'][-1]['output'][-1])\n",
        "    return (sentence1, sentence2, 'entailment')\n",
        "\n",
        "# Premise hypothesis - conversation\n",
        "def premise_hypothesis_conversation():\n",
        "    model_key = \"t0pp\"\n",
        "    model_parameters = {\n",
        "    \"text\": \"Generate a premise in conversation.\",\n",
        "    \"max_length\": 256,\n",
        "    }\n",
        "    out1 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence1 = process_sentences(out1['choices'][-1]['output'][-1])\n",
        "    model_parameters = {\n",
        "        \"text\": f\"Generate a hypothesis form the premise. Premise:{sentence1}\",\n",
        "        \"max_length\": 64,\n",
        "    }\n",
        "    out2 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence2 = process_sentences(out2['choices'][-1]['output'][-1])\n",
        "    return (sentence1, sentence2, 'entailment')\n",
        "\n",
        "# Premise hypothesis neutral\n",
        "def premise_hypothesis_neutral_normal():\n",
        "    model_key = \"t0pp\"\n",
        "    model_parameters = {\n",
        "    \"text\": \"Generate a premise\",\n",
        "    \"max_length\": 256,\n",
        "    }\n",
        "    out1 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence1 = process_sentences(out1['choices'][-1]['output'][-1])\n",
        "    model_parameters = {\n",
        "        \"text\": f\"Generate a random sentence from premise:{sentence1}\",\n",
        "        \"max_length\": 64,\n",
        "    }\n",
        "    out2 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence2 = process_sentences(out2['choices'][-1]['output'][-1])\n",
        "    return (sentence1, sentence2, 'neutral')\n",
        "\n",
        "# Premise hypothesis neutral\n",
        "def premise_hypothesis_neutral_conversation():\n",
        "    model_key = \"t0pp\"\n",
        "    model_parameters = {\n",
        "    \"text\": \"Generate a premise in conversation.\",\n",
        "    \"max_length\": 256,\n",
        "    }\n",
        "    out1 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence1 = process_sentences(out1['choices'][-1]['output'][-1])\n",
        "    model_parameters = {\n",
        "        \"text\": f\"Generate a random sentence from premise:{sentence1}\",\n",
        "        \"max_length\": 64,\n",
        "    }\n",
        "    out2 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence2 = process_sentences(out2['choices'][-1]['output'][-1])\n",
        "    return (sentence1, sentence2, 'neutral')\n",
        "\n",
        "# Premise hypothesis contradiction\n",
        "def premise_hypothesis_contradict_normal():\n",
        "    model_key = \"t0pp\"\n",
        "    model_parameters = {\n",
        "    \"text\": \"Generate a premise.\",\n",
        "    \"max_length\": 256,\n",
        "    }\n",
        "    out1 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence1 = process_sentences(out1['choices'][-1]['output'][-1])\n",
        "    model_parameters = {\n",
        "        \"text\": f\"Generate hypothesis from the premise. Premise: {sentence1}\",\n",
        "        \"max_length\": 64,\n",
        "    }\n",
        "    out2 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence2 = process_sentences(out2['choices'][-1]['output'][-1])\n",
        "    model_parameters = {\n",
        "        \"text\": f\"Generate a contradictory sentence for : {sentence2}\",\n",
        "        \"max_length\": 64,\n",
        "    }\n",
        "    out3 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence3 = out3['choices'][-1]['output'][-1]\n",
        "    return (sentence1, sentence2, sentence3, 'contradiction')\\\n",
        "\n",
        "def premise_hypothesis_contradict_conversation():\n",
        "    model_key = \"t0pp\"\n",
        "    model_parameters = {\n",
        "    \"text\": \"Generate a random fact:\",\n",
        "    \"max_length\": 256,\n",
        "    }\n",
        "    out1 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence1 = process_sentences(out1['choices'][-1]['output'][-1])\n",
        "    model_parameters = {\n",
        "        \"text\": f\"Generate hypothesis from the premise. Premise: {sentence1}\",\n",
        "        \"max_length\": 64,\n",
        "    }\n",
        "    out2 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence2 = process_sentences(out2['choices'][-1]['output'][-1])\n",
        "    model_parameters = {\n",
        "        \"text\": f\"Generate a contradictory sentence for: {sentence2}\",\n",
        "        \"max_length\": 64,\n",
        "    }\n",
        "    out3 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence3 = out3['choices'][-1]['output'][-1]\n",
        "    return (sentence1, sentence2, sentence3, 'contradiction')\n",
        "\n",
        "\n",
        "def main():\n",
        "    write_header = True\n",
        "    QQP_header = ['Premise', 'Hypothesis', 'Label']\n",
        "    QQP_csv_filename = \"premise_hypo_dataset.csv\"\n",
        "    if os.path.exists(QQP_csv_filename):\n",
        "        write_header = False\n",
        "    with open(QQP_csv_filename, 'a', newline='') as csv_file_descriptor:\n",
        "        csv_writer = csv.writer(csv_file_descriptor)\n",
        "        if write_header:\n",
        "            csv_writer.writerow(QQP_header)\n",
        "        for _ in tqdm(range(1000)):\n",
        "            s1, s2, state = premise_hypothesis_normal()\n",
        "            if SequenceMatcher(None, s1, s2).ratio() != 1.0:\n",
        "                csv_writer.writerow([s1, s2, state])\n",
        "            s1, s2, state = premise_hypothesis_conversation()\n",
        "            if SequenceMatcher(None, s1, s2).ratio() != 1.0:\n",
        "                csv_writer.writerow([s1, s2, state])\n",
        "            s1, s2, state = premise_hypothesis_neutral_normal()\n",
        "            if SequenceMatcher(None, s1, s2).ratio() != 1.0:\n",
        "                csv_writer.writerow([s1, s2, state])\n",
        "            s1, s2, state = premise_hypothesis_neutral_conversation()\n",
        "            if SequenceMatcher(None, s1, s2).ratio() != 1.0:\n",
        "                csv_writer.writerow([s1, s2, state])\n",
        "            s1, s2, s3, state = premise_hypothesis_contradict_normal()\n",
        "            if SequenceMatcher(None, s2, s3).ratio() != 1.0:\n",
        "                csv_writer.writerow([s1, s3, state])\n",
        "            s1, s2, s3, state = premise_hypothesis_contradict_conversation()\n",
        "            if SequenceMatcher(None, s2, s3).ratio() != 1.0:\n",
        "                csv_writer.writerow([s1, s3, state])\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwLwaIkt3BO6"
      },
      "source": [
        "# Question question quora.\n",
        "\n",
        "def generate_q_q_pair():\n",
        "    model_key = \"t0pp\"\n",
        "    model_parameters = {\n",
        "    \"text\": \"Generate a random long question.\",\n",
        "    \"max_length\": 256,\n",
        "    }\n",
        "    out1 = banana.run(banana_api, model_key, model_parameters)\n",
        "    q1 = out1['choices'][-1]['output'][-1]\n",
        "    model_parameters = {\n",
        "        \"text\" : f\"Paraphrase: {q1}\",\n",
        "        \"max_length\": 256,\n",
        "    }\n",
        "    out2 = banana.run(banana_api, model_key, model_parameters)\n",
        "    q2 = out2['choices'][-1]['output'][-1]\n",
        "    model_parameters = {\n",
        "        \"text\" : f'Generate a question on: {q2}',\n",
        "        \"max_length\": 256,\n",
        "    }\n",
        "    out3 = banana.run(banana_api, model_key, model_parameters)\n",
        "    q3 = out3['choices'][-1]['output'][-1]\n",
        "\n",
        "    return (q1, q3)\n",
        "\n",
        "def generate_random_question_pair():\n",
        "    model_key = \"t0pp\"\n",
        "    model_parameters = {\n",
        "    \"text\": \"Generate a random question.\",\n",
        "    \"max_length\": 120,\n",
        "    }\n",
        "    out1 = banana.run(banana_api, model_key, model_parameters)\n",
        "    q1 = out1['choices'][-1]['output'][-1]\n",
        "    out2 = banana.run(banana_api, model_key, model_parameters)\n",
        "    q2 = out2['choices'][-1]['output'][-1]\n",
        "    return (q1, q2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpMqrq4D4m43"
      },
      "source": [
        "generate_q_q_pair()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8J61INCxTIP"
      },
      "source": [
        "!pip install tdqm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-NLQR6mxaX-"
      },
      "source": [
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWzrIoBYs1lt"
      },
      "source": [
        "# QQ header\n",
        "def generate_question_question_dataset():\n",
        "    write_header = True\n",
        "    QQP_header = ['question1', 'question2', 'is_duplicate']\n",
        "    QQP_csv_filename = \"Question_Question_dataset.csv\"\n",
        "    if os.path.exists(QQP_csv_filename):\n",
        "        write_header = False\n",
        "    with open(QQP_csv_filename, 'a', newline='') as csv_file_descriptor:\n",
        "        csv_writer = csv.writer(csv_file_descriptor)\n",
        "        if write_header:\n",
        "            csv_writer.writerow(QQP_header)\n",
        "        for i in tqdm(range(10000)):\n",
        "            q1, q2 = generate_q_q_pair()\n",
        "            if q1 and q2 and q2.endswith('?'):\n",
        "                csv_writer.writerow([q1, q2, True])\n",
        "            q3, q4 = generate_random_question_pair()\n",
        "            if q3 and q4: \n",
        "                csv_writer.writerow([q3, q4, False])\n",
        "    print(\"Kekl question answer done!!!\")\n",
        "\n",
        "generate_question_question_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-pEoOf4_Mwk"
      },
      "source": [
        "# gramatical and ungramatical Gpt3 davince.\n",
        "# Tried T0_pp and GPT-3 davince, wasn;t able to generate ungrammatical sentences."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWaomq3E3mAW"
      },
      "source": [
        "def generate_correct_incorrect_pairs():\n",
        "    model_key = \"t0pp\"\n",
        "    model_parameters = {\n",
        "    \"text\": \"Generate a sentences with comma seperation.\",\n",
        "    \"max_length\": 120,\n",
        "    }\n",
        "    out1 = banana.run(banana_api, model_key, model_parameters)\n",
        "    sentence1 = out1['choices'][-1]['output'][-1]\n",
        "    return (sentence1, sentence2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDAy1dRcKG3M"
      },
      "source": [
        "print(generate_correct_incorrect_pairs())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lkNhGRrKLDi"
      },
      "source": [
        "from transformers import pipeline\n",
        "generator = pipeline(\"text-generation\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmgPmY1jkxea",
        "outputId": "c1c45fc0-9bc8-484d-b68b-96d32890a9e2"
      },
      "source": [
        "generator(\"generator related sentences with a comma separator.\") # Clearly didn't work"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'generator related sentences with a comma separator. You may also use a space.\\n\\nExample #2:\\n\\na.foo{...}\\n\\nand c.foo{foo}\\n\\nexample #3:\\n\\na.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM0yLY2qzo26"
      },
      "source": [
        "def clean_string(s):\n",
        "    if not s: return s\n",
        "    new_s = s.split('\\n')\n",
        "    s_list = []\n",
        "    for x in new_s:\n",
        "        if x.endswith('.'): s_list.append(x)\n",
        "        elif len(x.split('.')) > 1:\n",
        "            s_list.append('.'.join(x.split('.')[:-1]) + '.')\n",
        "        else:\n",
        "            s_list.append(x + '.')\n",
        "    new_s = s_list\n",
        "    if new_s[0][0].islower():\n",
        "        new_s[0] = new_s[0][0].upper() + new_s[0][1:]\n",
        "    return ' '.join(set(new_s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7H4BPx0pxGr"
      },
      "source": [
        "!pip install openai"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okj_WpIYo-1_"
      },
      "source": [
        "import os\n",
        "import openai\n",
        "\n",
        "openai.api_key = {Open-ai-key}\n",
        "\n",
        "def generate_with_gpt3(engine, prompt):\n",
        "    response = openai.Completion.create(\n",
        "        engine=engine,\n",
        "        prompt=prompt,\n",
        "        temperature=0.60,\n",
        "        max_tokens=32,\n",
        "        top_p=0.75,\n",
        "        frequency_penalty=0.75,\n",
        "        presence_penalty=0.5\n",
        "    )\n",
        "    return clean_string(response.choices[0].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUnFYr7lzX5i"
      },
      "source": [
        "# Generate all sentences using gpt3 ada, we can iter later using the the text file.\n",
        "from tqdm.notebook import tqdm\n",
        "with open(\"sentence_list_2.txt\", 'a') as fd:\n",
        "    for _ in tqdm(range(10000)):\n",
        "        text = generate_with_gpt3('davinci-instruct-beta-v3', \"Generate a long sentences on interesting topic.\")\n",
        "        if text:\n",
        "            new_text = generate_with_gpt3('davinci-instruct-beta-v3', f\"Given a sentence, introduce grammatical error. Sentece:{text}\")\n",
        "            fd.write(new_text + '\\n')\n",
        "    \n",
        "    # Changed different prompts it didn't work.\n",
        "    # for _ in tqdm(range(2000)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Generate a cause effect sentence.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n')\n",
        "    # for _ in tqdm(range(200)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Generate a sentence with random mood.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n')\n",
        "    # for _ in tqdm(range(5000)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Give a sentence from wikipedia.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n') \n",
        "    # for _ in tqdm(range(3000)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Narrate a random incident.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n') \n",
        "    # for _ in tqdm(range(8000)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Give a complex sentence.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n')\n",
        "    # for _ in tqdm(range(500)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Give a long sentence with a lot of expression.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n')\n",
        "    # for _ in tqdm(range(500)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Summarise a random historical event.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n')\n",
        "    # for _ in tqdm(range(500)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Describe some random event.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n')\n",
        "    # for _ in tqdm(range(200)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Give a conclusion to random story.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n')\n",
        "    # for _ in tqdm(range(200)):\n",
        "    #     text = generate_with_gpt3('davinci-instruct-beta-v3', \"Describe a day in random profession.\")\n",
        "    #     if text:\n",
        "    #         fd.write(text + '\\n')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLglWdSgBiHF"
      },
      "source": [
        "# Using text generated for QQ_pair and MNLI dataset, I'm manualy introducing the errors.\n",
        "# Ran this on local machine."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aCIgoxLpEnA9"
      },
      "source": [
        "import random\n",
        "import os\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "tense_list = \"awake awoke awoken be were been bear bore born beat beat beat become became become begin began begun \" \\\n",
        "             \"bend bent bent beset beset beset bet bet bet bid bade bid bind bound bound bite bit bitten bleed bled \" \\\n",
        "             \"bled blow blew blown break broke broken breed bred bred bring brought brought broadcast broadcast \" \\\n",
        "             \"broadcast build built built burn burned burned burst burst burst buy bought bought cast cast cast catch \" \\\n",
        "             \"caught caught choose chose chosen cling clung clung come came come cost cost cost creep crept crept cut \" \\\n",
        "             \"cut cut deal dealt dealt dig dug dug dive dove dived do did done draw drew drawn dream dreamt dreamed \" \\\n",
        "             \"drive drove driven drink drank drunk eat ate eaten fall fell fallen feed fed fed feel felt felt fight \" \\\n",
        "             \"fought fought find found found fit fit fit flee fled fled fling flung flung fly flew flown forbid \" \\\n",
        "             \"forbade forbidden forget forgot forgotten forgo foregone foregone forgive forgave forgiven forsake \" \\\n",
        "             \"forsook forsaken freeze froze frozen get got gotten give gave given go went gone grind ground ground \" \\\n",
        "             \"grow grew grown hang hung hung hear heard heard hide hid hidden hit hit hit hold held held hurt hurt \" \\\n",
        "             \"hurt keep kept kept kneel knelt knelt knit knit knit know knew know lay laid laid lead led led leap \" \\\n",
        "             \"leaped leapt learn learned learned leave left left lend lent lent let let let lie lay lain light lit \" \\\n",
        "             \"lighted lose lost lost make made made mean meant meant meet met met misspell misspelled misspelled \" \\\n",
        "             \"mistake mistook mistaken mow mowed mown overcome overcame overcome overdo overdid overdone overtake \" \\\n",
        "             \"overtook overtaken overthrow overthrew overthrown pay paid paid plead pled pled prove proved proven put \" \\\n",
        "             \"put put quit quit quit read read read rid rid rid ride rode ridden ring rang rung rise rose risen run \" \\\n",
        "             \"ran run saw sawed sawn say said said see saw seen seek sought sought sell sold sold send sent sent set \" \\\n",
        "             \"set set sew sewed sewn shake shook shaken shave shaved shaven shear shore shorn shed shed shed shine \" \\\n",
        "             \"shone shone shoe shoed shod shoot shot shot show showed shown shrink shrank shrunk shut shut shut sing \" \\\n",
        "             \"sang sung sink sank sunk sit sat sat sleep slept slept slay slew slain slide slid slid sling slung \" \\\n",
        "             \"slung slit slit slit smite smote smitten sow sowed sown speak spoke spoken speed sped sped spend spent \" \\\n",
        "             \"spent spill spilt spilled spin spun spun spit spat spit split split split spread spread spread spring \" \\\n",
        "             \"sprang sprung stand stood stood steal stole stolen stick stuck stuck sting stung stung stink stank \" \\\n",
        "             \"stunk stride strod stridden strike struck struck string strung strung strive strove striven swear swore \" \\\n",
        "             \"sworn sweep swept swept swell swelled swelled swim swam swum swing swung swung take took taken teach \" \\\n",
        "             \"taught taught tear tore torn tell told told think thought thought thrive thrived/throve thrived throw \" \\\n",
        "             \"threw thrown thrust thrust thrust tread trod trodden understand understood understood uphold upheld \" \\\n",
        "             \"upheld upset upset upset wake woke woken wear wore worn weave weaved woven wed wed wed weep wept wept \" \\\n",
        "             \"wind wound wound win won won withhold withheld withheld withstand withstood withstood wring wrung wrung \" \\\n",
        "             \"write wrote written \"\n",
        "tense = zip(*(iter(tense_list.split(\" \")),) * 3)\n",
        "tense_dict = dict()\n",
        "for x, y, z in tense:\n",
        "    tense_dict[x] = [y, z]\n",
        "    tense_dict[y] = [x, z]\n",
        "    tense_dict[z] = [x, y]\n",
        "tense_set = set(tense_dict.keys())\n",
        "\n",
        "articles = ['a', 'an', 'the']\n",
        "\n",
        "random_character = \"! & _ - : ; ? / . , ' \\\"\".split()\n",
        "\n",
        "pronouns_table = [\"I me mine my myself\", \"you yours your yourself\", \"she her hers herself\", \"he his him himself\",\n",
        "                  \"they them their theirs themself\", \"it its itself\", \"we us ours our ourselves\", \"one one's oneself\"]\n",
        "pronoun_dict = dict()\n",
        "for pronouns in pronouns_table:\n",
        "    pronoun_list = pronouns.split(' ')\n",
        "    for i in range(len(pronoun_list)):\n",
        "        pronoun_dict[pronoun_list[i]] = pronoun_list[:i] + pronoun_list[i+1:]\n",
        "pronoun_keys = set(pronoun_dict.keys())\n",
        "\n",
        "singular_plural_verb_pairs = \"agrees agree bakes bake calls call counts count drops drop enjoys enjoy forces force \" \\\n",
        "                             \"invents invent makes make points point pulls pull stirs stir uses use waves wave \" \\\n",
        "                             \"attaches attach catches catch crashes crash fixes fix guesses guess itches itch kisses \" \\\n",
        "                             \"kiss pitches pitch pushes push taxes tax teaches teach waltzes waltz washes wash wishes \" \\\n",
        "                             \"wish annoys annoy brays bray buys buy delays delay employes employ lays lay obeys obey \" \\\n",
        "                             \"pays pay plays play prays pray says say sprays spray stays stay sways sway applies \" \\\n",
        "                             \"apply bullies bully carries carry cries cry defies defy dries dry flies fly hurries \" \\\n",
        "                             \"hurry marries marry pitties pity replies reply spies spy try tries worries worry \"\n",
        "singular_plural_verb_pairs_list = zip(*(iter(singular_plural_verb_pairs.split(\" \")),) * 2)\n",
        "singular_plural_verb_dict = dict()\n",
        "\n",
        "for x, y in singular_plural_verb_pairs_list:\n",
        "    singular_plural_verb_dict[x] = y\n",
        "    singular_plural_verb_dict[y] = x\n",
        "singular_plural_verb_set = set(singular_plural_verb_dict.keys())\n",
        "\n",
        "singular_plural_noun_pairs = \"Army-Armies Ass-Asses Baby-Babies Bamboo-Bamboos Bench-Benches Bird-Birds Boat-Boats \" +\\\n",
        "                             \"Bone-Bones Box-Boxes Boy-Boys Buffalo-Buffaloes Bus-Buses Bush-Bushes Caddy-Caddies \" +\\\n",
        "                             \"Calf-Calves Car-Cars Cat-Cats Chair-Chairs Chief-Chiefs Child-Children City-Cities \" +\\\n",
        "                             \"Class-Classes Class fellow-Class fellows Cliff-Cliffs Clutch-Clutches Copy-Copies \" +\\\n",
        "                             \"Country-Countries Cow-Cows Cry-Cries Cuckoo-Cuckoos Cup-Cups Day-Days Deci-Decies \" +\\\n",
        "                             \"Deer-Deers Dog-Dogs Donkey-Donkeys Dozen-Dozens Duty-Duties Essay-Essays \" +\\\n",
        "                             \"Family-Families Father-Fathers Fish-Fishes Fly-Flies Foot-Feet Fox-Foxes Gas-Gases \" +\\\n",
        "                             \"Glass-Glasses Grass-Grasses Hair-Hairs Half-Halves Hand-Hands Hero-Heroes Hoof-Hoofs \" +\\\n",
        "                             \"Horse-Horses House-Houses Inch-Inches Jar-Jars Key-Keys Knife-Knives \" +\\\n",
        "                             \"Lady-LadiesLass-Lasses Leaf-Leaves Leg-Legs Life-Lives Loaf-Loaves Loof-Loofs \" +\\\n",
        "                             \"Love-Loves Maidservant-Maidservants Man-Men Mango-Mangoes Monkey-Monkeys MotherMothers \" +\\\n",
        "                             \"Mouse- Mice News-News Ox-Oxen Pencil-Pencils Penny-Pennies Person-People Pitch-Pitches \" +\\\n",
        "                             \"Poetry-Poetries Potato-Potatoes Proof-Proofs Quiz-Quizzes Radio-Radios Ray-Rays \" +\\\n",
        "                             \"River-River  Scissor-Scissors Self-Selves Sheep-Sheeps Shop-Shops Sister-Sisters \" +\\\n",
        "                             \"Sky-Skies Spey- Spies Stepson-Stepsons Story-Stories Table-Tables Thief-Thieves \" +\\\n",
        "                             \"Tooth-Teeth Toy-Toys Trouser-Trousers Uncle-Uncles Watch-Watches Wife-Wives Wish-Wishes \" +\\\n",
        "                             \"Woman-Women\"\n",
        "singular_plural_noun_pairs_list = [x.split(\"-\") for x in singular_plural_noun_pairs.split(\" \") if len(x.split(\"-\")) == 2]\n",
        "singular_plural_noun_dict = dict()\n",
        "for x in singular_plural_noun_pairs_list:\n",
        "    singular_plural_noun_dict[x[0]] = x[1]\n",
        "    singular_plural_noun_dict[x[1]] = x[0]\n",
        "\n",
        "singular_plural_noun_set = set(singular_plural_verb_dict.keys())\n",
        "\n",
        "\n",
        "def change_tense(s):\n",
        "    words = [x for x in s.split(' ') if x in tense_set]\n",
        "    choices = random.randint(0, len(words)+1 // 2)\n",
        "    if not words or not choices: return s\n",
        "    words = random.choices(words, k = choices)\n",
        "    t = s\n",
        "    for w in words:\n",
        "        t = t.replace(' '+w+' ', ' '+random.choice(tense_dict[w])+' ', 2)\n",
        "    return t\n",
        "\n",
        "\n",
        "def change_articles(s):\n",
        "    t = s\n",
        "    article_count = len([x for x in s.split(' ') if x in articles])\n",
        "    for _ in range(random.randint(0, article_count)):\n",
        "        t = t.replace(' '+random.choice(articles)+' ', ' ' + random.choice(articles) + ' ')\n",
        "    return t\n",
        "\n",
        "\n",
        "def change_to_random_symbol(s):\n",
        "    t = s\n",
        "    characters = list(set([x for x in s if x in random_character]))\n",
        "    choices = random.randint(0, len(characters)+1 // 2)\n",
        "    if not characters or not choices: return s\n",
        "    characters = random.choices(characters, k = choices)\n",
        "    for c in characters:\n",
        "        t = t.replace(' '+c+' ', ' '+random.choice(random_character)+' ', 1)\n",
        "    return t\n",
        "\n",
        "\n",
        "def replace_pronoun(s):\n",
        "    t = s\n",
        "    s_pronoun = list(set([x for x in s.split(' ') if x in pronoun_keys]))\n",
        "    choices = random.randint(0, len(s_pronoun)+1 // 2)\n",
        "    if not s_pronoun or not choices: return s\n",
        "    s_pronoun = random.choices(s_pronoun, k = choices)\n",
        "    for pn in s_pronoun:\n",
        "        t = t.replace(' '+pn+' ', ' '+random.choice(pronoun_dict[pn])+' ', 1)\n",
        "    return t\n",
        "\n",
        "\n",
        "def switch_singular_plural_verb(s):\n",
        "    t = s\n",
        "    word_list = list(set([x for x in s.split(' ') if x in singular_plural_verb_set]))\n",
        "    choices = random.randint(0, len(word_list)+1 //2)\n",
        "    if not word_list or not choices: return s\n",
        "    word_list = random.choices(word_list, k = choices)\n",
        "    for w in word_list:\n",
        "        t = t.replace(w, ' '+singular_plural_verb_dict[w]+' ', 1)\n",
        "    return t\n",
        "\n",
        "\n",
        "def shuffle_words(s, max_shuffle=2):\n",
        "    s_list = s.split(' ')\n",
        "    for _ in range(max_shuffle):\n",
        "        i, j = random.choices(range(0, len(s_list) - 1), k=2)\n",
        "        s_list[i], s_list[j] = s_list[j], s_list[i]\n",
        "    return ' '.join(s_list)\n",
        "\n",
        "\n",
        "def make_everything_caps(s):\n",
        "    return s.upper()\n",
        "\n",
        "\n",
        "def make_everything_lower(s):\n",
        "    return s.lower()\n",
        "\n",
        "\n",
        "def remove_space_after_fullstop(s):\n",
        "    choices = random.randint(0, 4)\n",
        "    return s.replace('. ', '.', choices)\n",
        "\n",
        "\n",
        "def make_first_lower(s):\n",
        "    return s[0].lower() + s[1:]\n",
        "\n",
        "\n",
        "def change_fullstop_comma(s):\n",
        "    choices = random.randint(0, 2)\n",
        "    return s.replace('.', ',', choices)\n",
        "\n",
        "\n",
        "def change_comma_fullstop(s):\n",
        "    choices = random.randint(0, 2)\n",
        "    return s.replace(',', '.', choices)\n",
        "\n",
        "\n",
        "def remove_apostrophe(s):\n",
        "    choices = random.randint(0, 2)\n",
        "    return s.replace(\"'\", '', choices)\n",
        "\n",
        "\n",
        "def remove_commas(s):\n",
        "    choices = random.randint(0, 2)\n",
        "    return s.replace(',', '', choices)\n",
        "\n",
        "\n",
        "def remove_fullstop(s):\n",
        "    choices = random.randint(0, 2)\n",
        "    return s.replace('.', '', choices)\n",
        "\n",
        "\n",
        "function_list = [remove_fullstop, remove_commas, remove_apostrophe, change_comma_fullstop, change_fullstop_comma,\n",
        "                 make_first_lower, remove_space_after_fullstop, make_everything_lower, make_everything_caps,\n",
        "                 shuffle_words, switch_singular_plural_verb, replace_pronoun, change_to_random_symbol, change_articles,\n",
        "                 change_tense]\n",
        "\n",
        "MAX_CHOICES = 6\n",
        "HEADER = ['acceptability', 'sentence']\n",
        "CSV_FILE = \"grammer.csv\"\n",
        "\n",
        "with open('mix_sentence.txt', 'r') as fr:\n",
        "    write_header = True\n",
        "    edit_sentence = 1\n",
        "    if os.path.exists(CSV_FILE): write_header = False\n",
        "    with open('grammer.csv', 'a', newline='') as fw:\n",
        "        csv_writer = csv.writer(fw)\n",
        "        if write_header:\n",
        "            csv_writer.writerow(HEADER)\n",
        "        sentences = fr.readlines()\n",
        "        for sen in tqdm(range(len(sentences))):\n",
        "            my_sen = sentences[sen]\n",
        "            if not edit_sentence:\n",
        "                csv_writer.writerow([my_sen.rstrip(), edit_sentence])\n",
        "                edit_sentence = 1\n",
        "            else:\n",
        "                fun_choices = random.randint(2, MAX_CHOICES)\n",
        "                my_funcs = random.sample(function_list, k=fun_choices)\n",
        "                for fun in my_funcs:\n",
        "                    my_sen = fun(my_sen)\n",
        "                csv_writer.writerow([my_sen.rstrip(), edit_sentence])\n",
        "                edit_sentence = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r5DfJkNTSJ6d",
        "outputId": "b610e014-bf7f-4f0a-e8d3-09a31cd9c4a7"
      },
      "source": [
        "#### Aggregated utils used so far\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import hashlib\n",
        "from difflib import SequenceMatcher\n",
        "from langdetect import detect\n",
        "import random\n",
        "\n",
        "\n",
        "def count_unique(csv_file):\n",
        "    unique_set = set()\n",
        "    with open(csv_file, 'r', newline='') as csv_file_descriptor:\n",
        "        csv_reader = csv.reader(csv_file_descriptor)\n",
        "        next(csv_reader, None)  # skip the headers\n",
        "        count = 0\n",
        "        for [q1, q2, state] in csv_reader:\n",
        "            count += 2\n",
        "            unique_set.add(hashlib.sha256(q1.encode()))\n",
        "            unique_set.add(hashlib.sha256(q2.encode()))\n",
        "    \n",
        "    print(count, len(unique_set))\n",
        "\n",
        "# Extarct all both csv and make ungrammatic dataset\n",
        "def get_sentences():\n",
        "    endings = set('.\"\\'?!')\n",
        "    with open(\"mix_sentence.txt\", 'a') as fw:\n",
        "        line_list = []\n",
        "        with open(\"premise_hypo_dataset.csv\", 'r', newline='') as hypo_csv:\n",
        "            reader = csv.reader(hypo_csv)\n",
        "            for [p, h, _] in reader:\n",
        "                if len(p.split()) > 8:\n",
        "                    if p[-1] not in endings: p+='.'\n",
        "                    line_list.append(p[0].upper() + p[1:]+'\\n')\n",
        "                if len(h.split()) > 8:\n",
        "                    if h[-1] not in endings: h += '.'\n",
        "                    line_list.append(h[0].upper() + h[1:]+'\\n')\n",
        "        with open(\"Question_Question_dataset.csv\", 'r', newline='') as question_csv:\n",
        "            reader = csv.reader(question_csv)\n",
        "            for [p, h, _] in reader:\n",
        "                if len(p.split()) > 8:\n",
        "                    if p[-1] not in endings: p += '.'\n",
        "                    line_list.append(p[0].upper() + p[1:]+'\\n')\n",
        "                if len(h.split()) > 8:\n",
        "                    if h[-1] not in endings: h += '.'\n",
        "                    line_list.append(h[0].upper() + h[1:]+'\\n')\n",
        "        random.shuffle(line_list)\n",
        "        fw.writelines(line_list)\n",
        "\n",
        "def filter_same_hypothesis_premise(csv_file, new_csv_file):\n",
        "    with open(csv_file, 'r', newline='') as base_csv:\n",
        "        with open(new_csv_file, 'a', newline='') as new_csv:\n",
        "            reader = csv.reader(base_csv)\n",
        "            writer = csv.writer(new_csv)\n",
        "            for [p, h, label] in reader:\n",
        "                if p and h and (SequenceMatcher(None, p, h).ratio() != 1.0):\n",
        "                    writer.writerow([p, h, label])\n",
        "\n",
        "\n",
        "# Code to filter duplicates.\n",
        "def copy_unique(f1 ,f2):\n",
        "    count = 0\n",
        "    unique_set = set()\n",
        "    with open(f1, 'r') as fr:\n",
        "        with open(f2, 'a') as fw:\n",
        "            for line in fr.readlines():\n",
        "                line.replace('. .', '.')\n",
        "                count += 1\n",
        "                hash_k = hashlib.sha256(line.encode()).hexdigest()\n",
        "                if hash_k in unique_set: continue\n",
        "                unique_set.add(hash_k)\n",
        "                try:\n",
        "                    if detect(line) == 'en':\n",
        "                        fw.write(line)\n",
        "                except:\n",
        "                    continue\n",
        "    print(count, len(unique_set))\n",
        "\n",
        "\n",
        "# Code to remove bad line endings\n",
        "def remove_bad_line_endings(f1, f2):\n",
        "    with open(f1, 'r') as fr:\n",
        "        with open(f2, 'w') as fw:\n",
        "            count = 0\n",
        "            for line in fr.readlines():\n",
        "                count += 1\n",
        "                write_line = line.lstrip()\n",
        "                write_line = write_line.rstrip() + '\\n'\n",
        "                fw.write(write_line)\n",
        "    print(count)\n",
        "\n",
        "\n",
        "def QQ_restructure_to_orginal_dataset():\n",
        "    row_id = 0\n",
        "    question_count = 0\n",
        "    OLD_CSV = \"Question_Question_dataset.csv\"\n",
        "    NEW_CSV = \"MY_QQP_dataset.csv\"\n",
        "    NEW_HEADER = ['id', 'qi1', 'qid2', 'question1', 'question2', 'is_duplicate']\n",
        "    with open(NEW_CSV, 'a') as fw:\n",
        "        csv_writer = csv.writer(fw)\n",
        "        csv_writer.writerow(NEW_HEADER)\n",
        "        with open(OLD_CSV, 'r') as fr:\n",
        "            csv_reader = csv.reader(fr)\n",
        "            csv_reader.__next__()\n",
        "            for [q1, q2, is_d] in csv_reader:\n",
        "                d = 1 if is_d == 'True' else 0\n",
        "                csv_writer.writerow([row_id, question_count, question_count+1, q1, q2, d])\n",
        "                row_id += 1\n",
        "                question_count += 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "70086 70086\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LwbhZAaU2E5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}